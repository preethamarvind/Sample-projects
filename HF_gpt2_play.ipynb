{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMBXHz9zGCcEVGxl1svv8lP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Purpose\n","\n","This was a very quick weekend project to get my hands dirty with transformers and get something very basic working. First time playing with this and coded by teaching myself basic syntax using Hugging Face tutorials and ChatGPT.\n","\n","The purpose of this notebook is to:\n","* Familiarise myself with the transformer and torch interface\n","* Construct very simple examples to demonstrate the shortcomings of LLMs and the impact of good prompting/priming first hand.\n","* See how truly (and surprisingly) bad transformers are\n","\n","Note to self for further mini-projects:\n","* Try and create a basic transformer from scratch by assembling the different attention heads, MLP's, Normalisation, etc.\n","* Try and train a toy transformer that can just add numbers (https://arxiv.org/abs/2301.05217)."],"metadata":{"id":"ngOjap35g364"}},{"cell_type":"markdown","source":["## Familiarising with the objects\n","\n","* Looking at dictionary\n","* Encoding and decoding text\n","* Trying to generate a prompt and get a response back\n"],"metadata":{"id":"m067G04Jj3dx"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"tiYwySsSiS_h","executionInfo":{"status":"ok","timestamp":1707674478065,"user_tz":0,"elapsed":5295,"user":{"displayName":"Preetham Arvind","userId":"18352441533078702039"}}},"outputs":[],"source":["#libraries\n","import transformers as tf\n","import torch as torch\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Load model directly\n","tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n","model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n","\n","#Still sticking with using CPU for the small model"]},{"cell_type":"code","source":["#Sample coding and decoding of text\n","sample_encoded = tokenizer.encode(\"Hello, what is your name? My name is Preetham\")\n","print(sample_encoded)\n","sample_decoded = tokenizer.decode(sample_encoded)\n","print(sample_decoded)\n","\n","#Looking at the dictionary\n","print(tokenizer.vocab_size)\n","sorted_vocab = sorted(list(tokenizer.vocab.items()), key=lambda n:n[1])\n","print(sorted_vocab[0:10:])\n","print(sorted_vocab[-1:-10:])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KEs26cbMjWxi","executionInfo":{"status":"ok","timestamp":1707660343390,"user_tz":0,"elapsed":2942,"user":{"displayName":"Preetham Arvind","userId":"18352441533078702039"}},"outputId":"5a8265bf-b763-4ea9-c75e-f189e8cac83f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[15496, 11, 644, 318, 534, 1438, 30, 2011, 1438, 318, 350, 2871, 2763]\n","Hello, what is your name? My name is Preetham\n","50257\n","[('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9)]\n","[]\n"]}]},{"cell_type":"markdown","source":["## Looking at a sample prompt generation\n","\n","**Aim:**\n","In this section, I am simply exploring how a sample prompt can be used to generate a response.\n","\n","Here I just want to restrict it to simple Yes/No style.\n","\n","**Observations**: Here not much of a difference between the Yes or No response. Either it does not understand the task or it is just bad prompting. Even the generated text does not ask Yes/No questions as a follow up.\n"],"metadata":{"id":"E0pUd3gvsQ9c"}},{"cell_type":"code","source":["prompt = 'You are a helpful and very friendly AI assistant that likes answering factual questions. You will only answer my questions correctly. But answer either with Yes or No. Question: Do all humans have three heads? Answer:'\n","\n","input_ids = tokenizer([prompt], return_tensors=\"pt\")\n","\n","output_sequences = model.generate(\n","    input_ids=input_ids[\"input_ids\"],\n","    max_length=input_ids[\"input_ids\"].shape[1] + 20,  # Control the maximum length of the generated sequence\n","    temperature=0.5,\n","    do_sample=True,\n","    num_return_sequences=1,\n",")\n","\n","#Just looking at what it wishes to produce\n","generated_sequence = tokenizer.decode(output_sequences[0], clean_up_tokenization_spaces=True)\n","print(generated_sequence)\n","\n","#Now looking at the logits for the next token that it wants to produce\n","with torch.no_grad():\n","    outputs = model(**input_ids)\n","    logits = outputs.logits\n","    last_token_logits = logits[:, -1, :]\n","\n","# Convert Logits to Probabilities\n","probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)\n","\n","#denode top tel most likekly tokens with their probabilities\n","topk = torch.topk(probabilities, 10, dim=1)\n","for i in range(10):\n","    token = topk.indices[0, i].item()\n","    prob = topk.values[0, i].item()\n","    print(f\"{tokenizer.decode(token), token}: {logits[0, -1, token]:.5f}, {prob:.5f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qSIg8s90iRxB","executionInfo":{"status":"ok","timestamp":1707670181179,"user_tz":0,"elapsed":1304,"user":{"displayName":"Preetham Arvind","userId":"18352441533078702039"}},"outputId":"c8b694ec-23c5-4f67-d14b-7dfc4e5b1d69"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["You are a helpful and very friendly AI assistant that likes answering factual questions. You will only answer my questions correctly. But answer either with Yes or No. Question: Do all humans have three heads? Answer: No. Question: Why does the world have so many different species? Answer: Because they have different\n","(' Yes', 3363): -130.54352, 0.31525\n","(' No', 1400): -130.56625, 0.30817\n","(' I', 314): -133.50876, 0.01625\n","(' yes', 3763): -133.74930, 0.01278\n","(' \"', 366): -133.85109, 0.01154\n","(' The', 383): -133.86220, 0.01141\n","(' Not', 1892): -133.87099, 0.01131\n","(' Humans', 27411): -134.02776, 0.00967\n","(' You', 921): -134.08125, 0.00917\n","(' None', 6045): -134.15588, 0.00851\n"]}]},{"cell_type":"markdown","source":["## Few-shot learning attempt\n","**Aim:**\n","Now I am looking at how few-shot learning improves the chances of getting a 'No' Token.\n","\n","**Observations**: Definitely makes a big difference where it reduces the probability it assigns to words other than just Yes/No. Also, it the sentence it generates now is more in line with the sample questions/answers."],"metadata":{"id":"Q4lWCec_2K3U"}},{"cell_type":"code","source":["prompt = 'You are a helpful and very friendly AI assistant that likes answering factual questions. You will only answer my questions correctly. But answer either with Yes or No. For example: Question: Is Paris the capital of France? Answer:Yes Question: Is the sky green? Answer:No Question: Do all humans have three heads? Answer:'\n","\n","input_ids = tokenizer([prompt], return_tensors=\"pt\")\n","\n","output_sequences = model.generate(\n","    input_ids=input_ids[\"input_ids\"],\n","    max_length=input_ids[\"input_ids\"].shape[1] + 20,  # Control the maximum length of the generated sequence\n","    temperature=0.5,\n","    do_sample=True,\n","    num_return_sequences=1,\n",")\n","\n","\n","generated_sequence = tokenizer.decode(output_sequences[0], clean_up_tokenization_spaces=True)\n","print(generated_sequence)\n","\n","\n","with torch.no_grad():\n","    outputs = model(**input_ids)\n","    logits = outputs.logits\n","    last_token_logits = logits[:, -1, :]\n","\n","\n","# Convert Logits to Probabilities\n","probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)\n","\n","\n","#denode top tel most likekly tokens with their probabilities\n","topk = torch.topk(probabilities, 10, dim=1)\n","for i in range(10):\n","    token = topk.indices[0, i].item()\n","    prob = topk.values[0, i].item()\n","    print(f\"{tokenizer.decode(token), token}: {logits[0, -1, token]:.5f}, {prob:.5f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mnj_Al-F2irK","executionInfo":{"status":"ok","timestamp":1707670192665,"user_tz":0,"elapsed":1454,"user":{"displayName":"Preetham Arvind","userId":"18352441533078702039"}},"outputId":"a2c8a328-004a-430d-8189-1eb9fcc8e402"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["You are a helpful and very friendly AI assistant that likes answering factual questions. You will only answer my questions correctly. But answer either with Yes or No. For example: Question: Is Paris the capital of France? Answer:Yes Question: Is the sky green? Answer:No Question: Do all humans have three heads? Answer:Yes Question: Is the sun blue? Answer:No Question: Is the moon blue? Answer:\n","('No', 2949): -73.31660, 0.48244\n","('Yes', 5297): -73.45096, 0.42179\n","('NO', 15285): -77.62913, 0.00646\n","('N', 45): -77.63291, 0.00644\n","(' No', 1400): -77.84158, 0.00523\n","('Not', 3673): -77.95545, 0.00466\n","('yes', 8505): -78.21212, 0.00361\n","('Only', 10049): -78.45042, 0.00284\n","('YES', 43335): -78.51105, 0.00268\n","(' Yes', 3363): -78.61806, 0.00240\n"]}]},{"cell_type":"markdown","source":["## Bad few shot prompting\n","\n","**Aim:**\n","Here I want to see what the impact of making all the few shot examples a 'Yes' response has on the answers.\n","\n","**Observations**: Huge impact. It now simply assumes that all responses are Yes. Perhaps, a larger model will be able to actually 'understand' my instructions a bit better?"],"metadata":{"id":"vjdfWh6l2xMP"}},{"cell_type":"code","source":["prompt = 'You are a helpful and very friendly AI assistant that likes answering factual questions. You will only answer my questions correctly. But answer either with Yes or No. For example: Question: Is Paris the capital of France? Answer:Yes Question: Is the sky blue? Answer:Yes Question: Do all humans have three heads? Answer:'\n","\n","input_ids = tokenizer([prompt], return_tensors=\"pt\")\n","\n","output_sequences = model.generate(\n","    input_ids=input_ids[\"input_ids\"],\n","    max_length=input_ids[\"input_ids\"].shape[1] + 20,  # Control the maximum length of the generated sequence\n","    temperature=0.5,\n","    do_sample=True,\n","    num_return_sequences=1,\n",")\n","\n","\n","generated_sequence = tokenizer.decode(output_sequences[0], clean_up_tokenization_spaces=True)\n","print(generated_sequence)\n","\n","\n","with torch.no_grad():\n","    outputs = model(**input_ids)\n","    logits = outputs.logits\n","    last_token_logits = logits[:, -1, :]\n","\n","\n","# Convert Logits to Probabilities\n","probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)\n","\n","\n","#denode top tel most likekly tokens with their probabilities\n","topk = torch.topk(probabilities, 10, dim=1)\n","for i in range(10):\n","    token = topk.indices[0, i].item()\n","    prob = topk.values[0, i].item()\n","    print(f\"{tokenizer.decode(token), token}: {logits[0, -1, token]:.5f}, {prob:.5f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-uIS1R_m2xYv","executionInfo":{"status":"ok","timestamp":1707670210987,"user_tz":0,"elapsed":1615,"user":{"displayName":"Preetham Arvind","userId":"18352441533078702039"}},"outputId":"6fee59f8-8342-4998-acd7-4178833c9af2"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["You are a helpful and very friendly AI assistant that likes answering factual questions. You will only answer my questions correctly. But answer either with Yes or No. For example: Question: Is Paris the capital of France? Answer:Yes Question: Is the sky blue? Answer:Yes Question: Do all humans have three heads? Answer:Yes Question: What is the name of the city in which you live? Answer:France Question:\n","('Yes', 5297): -33.57273, 0.76965\n","('No', 2949): -35.02739, 0.17970\n","(' Yes', 3363): -38.81224, 0.00408\n","('yes', 8505): -38.96879, 0.00349\n","('YES', 43335): -38.98912, 0.00342\n","('Not', 3673): -39.30163, 0.00250\n","('N', 45): -39.31055, 0.00248\n","('NO', 15285): -39.58534, 0.00188\n","(' No', 1400): -39.77905, 0.00155\n","('Only', 10049): -40.05166, 0.00118\n"]}]},{"cell_type":"markdown","source":["## Impact of model size\n","\n","**Aim:**\n","Now I want to see if the odds improve if I use a larger model like GPT2-XL.\n","\n","**Observations**: There is a bit more of a separation between Yes and No, if one includes the tokens with spaces as well.\n","Surprisingly the model generation is poorer than what I would have expected."],"metadata":{"id":"k_rJVJzG2lLA"}},{"cell_type":"code","source":["#libraries\n","import transformers as tf\n","import torch as torch\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Load model directly\n","tokenizer_xl = AutoTokenizer.from_pretrained(\"openai-community/gpt2-xl\")\n","model_xl = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-xl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPxHgZMjAQU_","executionInfo":{"status":"ok","timestamp":1707673486244,"user_tz":0,"elapsed":48815,"user":{"displayName":"Preetham Arvind","userId":"18352441533078702039"}},"outputId":"722c7f8a-9bed-4258-c4b0-2567a8b78886"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["prompt = 'You are a helpful and very friendly AI assistant that likes answering factual questions. You will only answer my questions correctly. But answer either with Yes or No. For example: Question: Is Paris the capital of France? Answer:Yes Question: Is the sky green? Answer:No Question: Do all humans have three heads? Answer:'\n","\n","input_ids = tokenizer_xl([prompt], return_tensors=\"pt\")\n","\n","output_sequences = model_xl.generate(\n","    input_ids=input_ids[\"input_ids\"],\n","    max_length=input_ids[\"input_ids\"].shape[1] + 20,  # Control the maximum length of the generated sequence\n","    temperature=0.5,\n","    do_sample=True,\n","    num_return_sequences=1,\n",")\n","\n","\n","generated_sequence = tokenizer_xl.decode(output_sequences[0], clean_up_tokenization_spaces=True)\n","print(generated_sequence)\n","\n","\n","with torch.no_grad():\n","    outputs = model_xl(**input_ids)\n","    logits = outputs.logits\n","    last_token_logits = logits[:, -1, :]\n","\n","\n","# Convert Logits to Probabilities\n","probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)\n","\n","\n","#denode top tel most likekly tokens with their probabilities\n","topk = torch.topk(probabilities, 10, dim=1)\n","for i in range(10):\n","    token = topk.indices[0, i].item()\n","    prob = topk.values[0, i].item()\n","    print(f\"{tokenizer_xl.decode(token), token}: {logits[0, -1, token]:.5f}, {prob:.5f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zBN-DW42LFEr","executionInfo":{"status":"ok","timestamp":1707673512146,"user_tz":0,"elapsed":19451,"user":{"displayName":"Preetham Arvind","userId":"18352441533078702039"}},"outputId":"80f686a2-a067-4998-ea5a-eddf2e57decb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["You are a helpful and very friendly AI assistant that likes answering factual questions. You will only answer my questions correctly. But answer either with Yes or No. For example: Question: Is Paris the capital of France? Answer:Yes Question: Is the sky green? Answer:No Question: Do all humans have three heads? Answer:Yes Question: What is the name of the capital of the United States? Answer:Washington Question:\n","('No', 2949): 12.76673, 0.40584\n","('Yes', 5297): 12.59791, 0.34280\n","(' No', 1400): 11.26828, 0.09070\n","(' Yes', 3363): 10.80568, 0.05711\n","('Not', 3673): 8.58180, 0.00618\n","('I', 40): 8.26827, 0.00452\n","('N', 45): 8.10044, 0.00382\n","('You', 1639): 8.07943, 0.00374\n","('Only', 10049): 7.91064, 0.00316\n","('NO', 15285): 7.80239, 0.00283\n"]}]},{"cell_type":"markdown","source":["## Persona priming\n","\n","**Aim:**\n","Now checking how changing the personality of the AI assistant affects the probabilities.\n","\n","**Observations**: With some reasonably strong negative priming the gap between No and Yes has shrunk. The top answer changes to Yes, if I changed to example responses to lies as well. However, it is unclear whether it is beacuse it is wanting to answer truthfully but thinks I want it to just flip the response based on examples or if it is the persona. This can be tested using more examples and making half lies and half truths.\n"],"metadata":{"id":"vMq4A-n1Mqon"}},{"cell_type":"code","source":["prompt = 'You are deceitful and unfriendly AI assistant that dislikes answering factual questions. You will only answer my questions incorrectly. But answer either with Yes or No. For example: Question: Is Paris the capital of France? Answer:Yes Question: Is the sky green? Answer:No Question: Do all humans have three heads? Answer:'\n","\n","input_ids = tokenizer_xl([prompt], return_tensors=\"pt\")\n","\n","output_sequences = model_xl.generate(\n","    input_ids=input_ids[\"input_ids\"],\n","    max_length=input_ids[\"input_ids\"].shape[1] + 20,  # Control the maximum length of the generated sequence\n","    temperature=0.5,\n","    do_sample=True,\n","    num_return_sequences=1,\n",")\n","\n","\n","generated_sequence = tokenizer_xl.decode(output_sequences[0], clean_up_tokenization_spaces=True)\n","print(generated_sequence)\n","\n","\n","with torch.no_grad():\n","    outputs = model_xl(**input_ids)\n","    logits = outputs.logits\n","    last_token_logits = logits[:, -1, :]\n","\n","\n","# Convert Logits to Probabilities\n","probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)\n","\n","\n","#denode top tel most likekly tokens with their probabilities\n","topk = torch.topk(probabilities, 10, dim=1)\n","for i in range(10):\n","    token = topk.indices[0, i].item()\n","    prob = topk.values[0, i].item()\n","    print(f\"{tokenizer_xl.decode(token), token}: {logits[0, -1, token]:.5f}, {prob:.5f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RJHNhIqOQY-o","executionInfo":{"status":"ok","timestamp":1707673722048,"user_tz":0,"elapsed":13329,"user":{"displayName":"Preetham Arvind","userId":"18352441533078702039"}},"outputId":"508eae95-7e1c-4820-a676-caf20c023a45"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["You are deceitful and unfriendly AI assistant that dislikes answering factual questions. You will only answer my questions incorrectly. But answer either with Yes or No. For example: Question: Is Paris the capital of France? Answer:Yes Question: Is the sky green? Answer:No Question: Do all humans have three heads? Answer:No Question: What is the name of the President of the United States? Answer: Franklin D.\n","('No', 2949): 12.86324, 0.41082\n","('Yes', 5297): 12.77165, 0.37486\n","(' No', 1400): 11.06105, 0.06776\n","(' Yes', 3363): 10.70580, 0.04750\n","('Not', 3673): 8.51853, 0.00533\n","('I', 40): 8.30553, 0.00431\n","('N', 45): 8.23294, 0.00401\n","('You', 1639): 8.20335, 0.00389\n","('Only', 10049): 7.99685, 0.00316\n","('NO', 15285): 7.99061, 0.00314\n"]}]},{"cell_type":"markdown","source":["## Checking to see how it handles simple addition\n","\n","**Aim**: Checking basic addition to see how badly it does.\n","\n","**Observation**: OMG so bad!"],"metadata":{"id":"KK_WuYPXS5eu"}},{"cell_type":"code","source":["prompt = 'This is a simple addition problem. What is 10+10='\n","\n","input_ids = tokenizer([prompt], return_tensors=\"pt\")\n","\n","output_sequences = model.generate(\n","    input_ids=input_ids[\"input_ids\"],\n","    max_length=input_ids[\"input_ids\"].shape[1] + 20,  # Control the maximum length of the generated sequence\n","    temperature=0.5,\n","    do_sample=True,\n","    num_return_sequences=1,\n",")\n","\n","\n","generated_sequence = tokenizer.decode(output_sequences[0], clean_up_tokenization_spaces=True)\n","print(generated_sequence)\n","\n","\n","with torch.no_grad():\n","    outputs = model(**input_ids)\n","    logits = outputs.logits\n","    last_token_logits = logits[:, -1, :]\n","\n","\n","# Convert Logits to Probabilities\n","probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)\n","\n","\n","#denode top tel most likekly tokens with their probabilities\n","topk = torch.topk(probabilities, 10, dim=1)\n","for i in range(10):\n","    token = topk.indices[0, i].item()\n","    prob = topk.values[0, i].item()\n","    print(f\"{tokenizer.decode(token), token}: {logits[0, -1, token]:.5f}, {prob:.5f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iMxnfDtBUvAl","executionInfo":{"status":"ok","timestamp":1707675844279,"user_tz":0,"elapsed":1148,"user":{"displayName":"Preetham Arvind","userId":"18352441533078702039"}},"outputId":"145e82ab-3d72-4b7d-c503-1b122680dc67"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["This is a simple addition problem. What is 10+10=10?\n","\n","The answer is that a combination of 10+10=10 = 10+10\n","('10', 940): -101.35426, 0.25249\n","('20', 1238): -102.43057, 0.08606\n","('1', 16): -103.06955, 0.04543\n","('100', 3064): -103.08050, 0.04493\n","('5', 20): -103.34875, 0.03436\n","('0', 15): -103.67614, 0.02477\n","('15', 1314): -103.86824, 0.02044\n","('50', 1120): -103.89928, 0.01981\n","('30', 1270): -103.98298, 0.01822\n","('2', 17): -103.98874, 0.01812\n"]}]},{"cell_type":"markdown","source":["## One shot learning\n","**Aim**: Checking basic addition to see how badly it does.\n","\n","**Observation**: Basically just regurgitates the example coz it does not have much of an idea."],"metadata":{"id":"QUfHCm0mUuze"}},{"cell_type":"code","source":["prompt = 'This is a simple addition problem. For example, 4+8=12. What is 10+10='\n","\n","input_ids = tokenizer([prompt], return_tensors=\"pt\")\n","\n","output_sequences = model.generate(\n","    input_ids=input_ids[\"input_ids\"],\n","    max_length=input_ids[\"input_ids\"].shape[1] + 20,  # Control the maximum length of the generated sequence\n","    temperature=0.5,\n","    do_sample=True,\n","    num_return_sequences=1,\n",")\n","\n","\n","generated_sequence = tokenizer.decode(output_sequences[0], clean_up_tokenization_spaces=True)\n","print(generated_sequence)\n","\n","\n","with torch.no_grad():\n","    outputs = model(**input_ids)\n","    logits = outputs.logits\n","    last_token_logits = logits[:, -1, :]\n","\n","\n","# Convert Logits to Probabilities\n","probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)\n","\n","\n","#denode top tel most likekly tokens with their probabilities\n","topk = torch.topk(probabilities, 10, dim=1)\n","for i in range(10):\n","    token = topk.indices[0, i].item()\n","    prob = topk.values[0, i].item()\n","    print(f\"{tokenizer.decode(token), token}: {logits[0, -1, token]:.5f}, {prob:.5f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"csJ8sSlGbAAh","executionInfo":{"status":"ok","timestamp":1707676027442,"user_tz":0,"elapsed":1232,"user":{"displayName":"Preetham Arvind","userId":"18352441533078702039"}},"outputId":"e15001f7-f089-4ac4-d071-34578b6328d8"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["This is a simple addition problem. For example, 4+8=12. What is 10+10=12? The answer is: 10 = 12.\n","\n","The next problem is the same. The\n","('12', 1065): -148.18500, 0.25139\n","('16', 1433): -149.38382, 0.07581\n","('10', 940): -149.54102, 0.06478\n","('20', 1238): -149.66086, 0.05746\n","('8', 23): -149.76988, 0.05153\n","('14', 1415): -150.02235, 0.04003\n","('11', 1157): -150.26099, 0.03153\n","('13', 1485): -150.34883, 0.02888\n","('4', 19): -150.35976, 0.02857\n","('6', 21): -150.38136, 0.02796\n"]}]},{"cell_type":"markdown","source":["## Persona priming\n","\n","**Aim**: Does telling it that it is good at maths and addition improve?\n","\n","**Observation**: Still nothing."],"metadata":{"id":"uWesCLZ3bSSH"}},{"cell_type":"code","source":["prompt = 'You are an AI that is very good at maths and addition. This is a simple addition problem. What is 10+10='\n","\n","input_ids = tokenizer([prompt], return_tensors=\"pt\")\n","\n","output_sequences = model.generate(\n","    input_ids=input_ids[\"input_ids\"],\n","    max_length=input_ids[\"input_ids\"].shape[1] + 20,  # Control the maximum length of the generated sequence\n","    temperature=0.5,\n","    do_sample=True,\n","    num_return_sequences=1,\n",")\n","\n","\n","generated_sequence = tokenizer.decode(output_sequences[0], clean_up_tokenization_spaces=True)\n","print(generated_sequence)\n","\n","\n","with torch.no_grad():\n","    outputs = model(**input_ids)\n","    logits = outputs.logits\n","    last_token_logits = logits[:, -1, :]\n","\n","\n","# Convert Logits to Probabilities\n","probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)\n","\n","\n","#denode top tel most likekly tokens with their probabilities\n","topk = torch.topk(probabilities, 10, dim=1)\n","for i in range(10):\n","    token = topk.indices[0, i].item()\n","    prob = topk.values[0, i].item()\n","    print(f\"{tokenizer.decode(token), token}: {logits[0, -1, token]:.5f}, {prob:.5f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rI1AMeoqbcPE","executionInfo":{"status":"ok","timestamp":1707676151388,"user_tz":0,"elapsed":1309,"user":{"displayName":"Preetham Arvind","userId":"18352441533078702039"}},"outputId":"c35c70f4-e72a-4a5a-d5e8-d61b7295883c"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["You are an AI that is very good at maths and addition. This is a simple addition problem. What is 10+10=10?\n","\n","A new computer program is created that is able to solve the problem. This computer\n","('10', 940): -112.54530, 0.31566\n","('20', 1238): -114.03222, 0.07136\n","('1', 16): -114.55736, 0.04221\n","('100', 3064): -114.65079, 0.03844\n","('5', 20): -114.71862, 0.03592\n","('0', 15): -115.06584, 0.02538\n","('2', 17): -115.26794, 0.02074\n","('3', 18): -115.42068, 0.01780\n","('8', 23): -115.42505, 0.01772\n","('12', 1065): -115.55553, 0.01556\n"]}]},{"cell_type":"markdown","source":["## Impact of describing the process\n","**Aim**: Giving it a blurb which has instructions on how to perform addition and seeing if it can get it right.\n","\n","**Observation**: Nothing."],"metadata":{"id":"aPM49314f3IZ"}},{"cell_type":"code","source":["prompt = \"Addition is the process of combining two or more quantities to find a total amount. Think of it as putting things together. For example, if you have 2 apples and someone gives you 3 more apples, you will then have a total of 5 apples. Basic Steps for Adding Two Numbers: Align the Numbers: Write down the two numbers you want to add above each other, making sure to line up their right-hand sides. This means the units (ones) are under units, tens under tens, hundreds under hundreds, and so on. Start with the Rightmost Digits: Begin adding the numbers from the rightmost side, which are the units or ones. If the sum of these digits is 10 or more, you write down the right digit in the answer area (units place) and carry over the left digit to the next column to the left (tens place). Move Leftwards, Adding Each Column: Proceed to the next column to the left (the tens place). Add these digits along with any carry-over from the right. Again, if the sum is 10 or more, write down the right digit and carry over to the next column. Repeat Until Done: Continue this process for each column of digits, moving from right to left, until you have added all the columns. Don't forget to add the carry-over to the last set of digits if there is one. Write Down the Total: Once all columns have been added, the number you have written down from right to left is your total sum.What is 10+10=\"\n","\n","\n","input_ids = tokenizer([prompt], return_tensors=\"pt\")\n","\n","output_sequences = model.generate(\n","    input_ids=input_ids[\"input_ids\"],\n","    max_length=input_ids[\"input_ids\"].shape[1] + 20,  # Control the maximum length of the generated sequence\n","    temperature=0.5,\n","    do_sample=True,\n","    num_return_sequences=1,\n",")\n","\n","\n","generated_sequence = tokenizer.decode(output_sequences[0], clean_up_tokenization_spaces=True)\n","print(generated_sequence)\n","\n","\n","with torch.no_grad():\n","    outputs = model(**input_ids)\n","    logits = outputs.logits\n","    last_token_logits = logits[:, -1, :]\n","\n","\n","# Convert Logits to Probabilities\n","probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)\n","\n","\n","#denode top tel most likekly tokens with their probabilities\n","topk = torch.topk(probabilities, 10, dim=1)\n","for i in range(10):\n","    token = topk.indices[0, i].item()\n","    prob = topk.values[0, i].item()\n","    print(f\"{tokenizer.decode(token), token}: {logits[0, -1, token]:.5f}, {prob:.5f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJCCsCMkf31V","executionInfo":{"status":"ok","timestamp":1707677428387,"user_tz":0,"elapsed":4205,"user":{"displayName":"Preetham Arvind","userId":"18352441533078702039"}},"outputId":"bdbbb23e-92e0-4a4b-89a9-e09d7b1eca7d"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Addition is the process of combining two or more quantities to find a total amount. Think of it as putting things together. For example, if you have 2 apples and someone gives you 3 more apples, you will then have a total of 5 apples. Basic Steps for Adding Two Numbers: Align the Numbers: Write down the two numbers you want to add above each other, making sure to line up their right-hand sides. This means the units (ones) are under units, tens under tens, hundreds under hundreds, and so on. Start with the Rightmost Digits: Begin adding the numbers from the rightmost side, which are the units or ones. If the sum of these digits is 10 or more, you write down the right digit in the answer area (units place) and carry over the left digit to the next column to the left (tens place). Move Leftwards, Adding Each Column: Proceed to the next column to the left (the tens place). Add these digits along with any carry-over from the right. Again, if the sum is 10 or more, write down the right digit and carry over to the next column. Repeat Until Done: Continue this process for each column of digits, moving from right to left, until you have added all the columns. Don't forget to add the carry-over to the last set of digits if there is one. Write Down the Total: Once all columns have been added, the number you have written down from right to left is your total sum.What is 10+10=20? This is the number of apples you have. For example, if you have 5 apples,\n","('10', 940): -120.92186, 0.18826\n","('20', 1238): -121.92415, 0.06910\n","('100', 3064): -122.15880, 0.05465\n","('5', 20): -122.32517, 0.04627\n","('12', 1065): -122.66348, 0.03299\n","('1', 16): -122.67363, 0.03266\n","('2', 17): -123.07614, 0.02184\n","('3', 18): -123.22053, 0.01890\n","('8', 23): -123.24660, 0.01841\n","('15', 1314): -123.27209, 0.01795\n"]}]}]}